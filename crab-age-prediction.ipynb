{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":51983,"databundleVersionId":5884542,"sourceType":"competition"},{"sourceId":2834512,"sourceType":"datasetVersion","datasetId":1734027}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split , KFold\nfrom sklearn.preprocessing import MinMaxScaler , LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam , SGD\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.model_selection import GridSearchCV\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Read Dataset**","metadata":{}},{"cell_type":"code","source":"dftrain = pd.read_csv('/kaggle/input/playground-series-s3e16/train.csv')\ndftest = pd.read_csv('/kaggle/input/playground-series-s3e16/test.csv')\n\nprint('The dimension of dftrain',dftrain.shape)\nprint('The dimension of dftest',dftest.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftrain.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Cleaning**","metadata":{}},{"cell_type":"code","source":"missingtrain = dftrain.isnull().sum()\nmissingtest = dftest.isnull().sum()\nprint(missingtrain)\nprint(\"---------------------\")\nprint(missingtest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftrain.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"number of zero in train['Height'] :\", len(dftrain[dftrain['Height']==0]))\ndftrain['Height'] = dftrain['Height'].replace({0:0.348089})\ndftrain.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ndftrain['Sex'] = le.fit_transform(dftrain['Sex'])\ndftest['Sex'] = le.fit_transform(dftest['Sex'])\ndftrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dftrain.corr()\ncorr = dftrain.corr()\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr, annot=True, cmap='magma', fmt=\".2f\")\nplt.title(\"Correlation Coefficients\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Identify : X_Train & Y_Train**","metadata":{}},{"cell_type":"code","source":"X_Train = dftrain[['Shell Weight','Height','Diameter','Length']]\nY_Train = dftrain[['Age']]\n\nX_Train1 = dftrain[['Sex','Length','Diameter','Height','Weight','Shucked Weight','Viscera Weight','Shell Weight']]\nY_Train1 = dftrain[['Age']]\n\nTest = dftest[['Shell Weight','Height','Diameter','Length']]\nTest1 = dftest[['Sex','Length','Diameter','Height','Weight','Shucked Weight','Viscera Weight','Shell Weight']]\n# print(X_Train)\n# print(\"------------------------------------\")\n# print(Test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Normalize Data**","metadata":{}},{"cell_type":"code","source":"#normarlize ให้ค่าอยู่ระหว่าง 0 ถึง 1 \nscaler = MinMaxScaler()\nX_Train=pd.DataFrame(scaler.fit_transform(X_Train), index=X_Train.index, columns=X_Train.columns)\nTest=pd.DataFrame(scaler.fit_transform(Test), index=Test.index, columns=Test.columns)\n# print(X_Train)\n# print('------------------')\n# print(Test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Training & Evaluation**","metadata":{}},{"cell_type":"code","source":"LR_parameters = {'fit_intercept': [True, False]}\nLGB_parameters = {\n    'n_estimators': [100,500,1000],\n    'learning_rate': [0.01,0.001]\n}\nXGB_parameters = {\n    'n_estimators': [100,500,1000],\n    'learning_rate': [0.01,0.001]\n}\nRF_parameters = {\n    'n_estimators': [100,500,1000],\n    'max_depth': [3,10]\n}\n\nmlp = Sequential()\nmlp.add(Dense(128, activation='sigmoid', input_shape=(X_Train1.shape[1],)))\nmlp.add(Dense(64, activation='sigmoid'))\nmlp.add(Dense(1))\nmlp.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n\nmaeLRarr = []\nmaeLGBarr = []\nmaeXGBarr = []\nmaeRFarr = []\nmaeMLParr = []\n\ncv = KFold(n_splits = 10)\ni=0\nfor train_idx , test_idx in cv.split(X_Train1,Y_Train1):\n    xxTrain , xxTest = X_Train.iloc[train_idx] , X_Train.iloc[test_idx]\n    yyTrain , yyTest = Y_Train.iloc[train_idx] , Y_Train.iloc[test_idx]\n    \n    lr_grid_search = GridSearchCV(LinearRegression(), LR_parameters)\n    lr_grid_search.fit(xxTrain, yyTrain)\n    \n    best_params = lr_grid_search.best_params_\n    \n    lr = LinearRegression(**best_params)\n    lr.fit(xxTrain,yyTrain)\n    \n    pred = lr.predict(xxTest)\n    maeLR = np.sqrt(mean_absolute_error(yyTest, np.round(pred)))\n    maeLRarr.append(maeLR)\n    print('Fold', i, '==> LinearRegression of MAE is ==>', maeLR)\n    i=i+1\n\n# ----------------------------------------------------------------------------------------------------------------    \ncv = KFold(n_splits = 10)    \nk=0\nfor train_idx1 , test_idx1 in cv.split(X_Train1,Y_Train1):\n    xxTrain1 , xxTest1 = X_Train1.iloc[train_idx1] , X_Train1.iloc[test_idx1]\n    yyTrain1 , yyTest1 = Y_Train1.iloc[train_idx1] , Y_Train1.iloc[test_idx1]\n    \n#     ------------------------- LGBM Regressor -------------------------------------------------------------------\n    \n    lgb_grid_search = GridSearchCV(LGBMRegressor(objective='mae'), LGB_parameters, scoring='neg_mean_absolute_error')\n    lgb_grid_search.fit(xxTrain1, yyTrain1)\n\n    lgb_best_params = lgb_grid_search.best_params_\n    \n    lgb = LGBMRegressor(**lgb_best_params,early_stopping_rounds=500)\n    lgb.fit(xxTrain1,yyTrain1,eval_set=[(xxTrain1, yyTrain1), (xxTest1, yyTest1)],verbose=0)\n    \n    predLGB = lgb.predict(xxTest1)\n    maeLGB = np.sqrt(mean_absolute_error(yyTest1, np.round(predLGB)))\n    maeLGBarr.append(maeLGB)\n#     ------------------------- XGB Regressor -------------------------------------------------------------------\n    \n    xgb_grid_search = GridSearchCV(XGBRegressor(objective=\"reg:pseudohubererror\"), XGB_parameters, scoring='neg_mean_absolute_error')\n    xgb_grid_search.fit(xxTrain1, yyTrain1)\n    \n    xgb_best_params = xgb_grid_search.best_params_\n    \n    xgb = XGBRegressor(**xgb_best_params,early_stopping_rounds=500)\n    xgb.fit(xxTrain1,yyTrain1,eval_set=[(xxTrain1, yyTrain1), (xxTest1, yyTest1)],verbose=0)\n    \n    predXGB = xgb.predict(xxTest1)\n    maeXGB = np.sqrt(mean_absolute_error(yyTest1, np.round(predXGB)))\n    maeXGBarr.append(maeXGB)\n#     ------------------------- Random Forrest -------------------------------------------------------------------\n    \n    rf_grid_search = GridSearchCV(RandomForestRegressor(), RF_parameters, scoring='neg_mean_absolute_error')\n    yyTrain1_flattened = np.ravel(yyTrain1)\n    rf_grid_search.fit(xxTrain1, yyTrain1_flattened)\n    \n    rf_best_params = rf_grid_search.best_params_\n    \n    rf = RandomForestRegressor(**rf_best_params)\n    rf.fit(xxTrain1,yyTrain1_flattened)    \n    predRF = rf.predict(xxTest1)\n    maeRF = np.sqrt(mean_absolute_error(yyTest1, np.round(predRF)))\n    maeRFarr.append(maeRF)\n    \n#     ------------------------- MLP -------------------------------------------------------------------\n    \n    mlp.fit(xxTrain1, yyTrain1,batch_size=32, epochs=50,verbose=0)  \n    predMLP = mlp.predict(xxTest1)  \n    maeMLP = np.sqrt(mean_absolute_error(yyTest1, np.round(predMLP))) \n    maeMLParr.append(maeMLP)\n    \n    \n#     ------------------------------------------------------------------------------------------------   \n    print('Fold', k, '==> LGBM of MAE is ==>', maeLGB)\n    print('Fold', k, '==> XGBoost of MAE is ==>', maeXGB)\n    print('Fold', k, '==> MLP of MAE is ==>', maeMLP)\n    print('Fold', k, '==> RF of MAE is ==>', maeRF)\n    k=k+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lgb_best_params)\nprint(best_params)\nprint(xgb_best_params)\nprint(rf_best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_score = np.mean(maeLRarr)\nLGB_score = np.mean(maeLGBarr)\nXGB_score = np.mean(maeXGBarr)\nRF_score = np.mean(maeRFarr)\nMLP_score = np.mean(maeMLParr)\n\nmodel_perf = pd.DataFrame({'Model': ['Linear Regression' ,'XGBRegressor','LGBMRegressor', 'Random Forrest', 'Multi Layer Perceptron'],'mae-score': [LR_score, XGB_score,LGB_score, RF_score, MLP_score]})\nplt.figure(figsize = (8, 8))\nax = sns.barplot(y = 'Model', x = 'mae-score', data = model_perf)\nax.bar_label(ax.containers[0]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predict Submission & Create Pickle File**","metadata":{}},{"cell_type":"code","source":"id = dftest['id']\nfinal1 = np.round(lr.predict(Test))\nfinal2 = np.round(xgb.predict(Test1))\nfinal3 = np.round(lgb.predict(Test1))\nfinal4 = np.round(mlp.predict(Test1))\nfinal5 = np.round(rf.predict(Test1))\n\nprint(final1.shape)\nfinal1 = final1.reshape(49368)\nprint(final1.shape)\n\nprint(final4.shape)\nfinal4 = final4.reshape(49368)\nprint(final4.shape)\n\noutput1 = pd.DataFrame({'id': id,'Age': final1.astype(int)})\noutput2 = pd.DataFrame({'id': id,'Age': final2.astype(int)})\noutput3 = pd.DataFrame({'id': id,'Age': final3.astype(int)})\noutput4 = pd.DataFrame({'id': id,'Age': final4.astype(int)})\noutput5 = pd.DataFrame({'id': id,'Age': final5.astype(int)})\n\noutput1.to_csv('LinearRegression_submission.csv', index=False)\noutput2.to_csv('XGB_submission.csv', index=False)\noutput3.to_csv('LGB_submission.csv', index=False)\noutput4.to_csv('MLP_submission.csv', index=False)\noutput5.to_csv('RF_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"LinearRegression.pkl\", 'wb') as file:\n    pickle.dump(lr, file)\n    \nwith open(\"XGBRegression.pkl\", 'wb') as file:\n    pickle.dump(xgb, file)\n\nwith open(\"LGBMRegression.pkl\", 'wb') as file:\n    pickle.dump(lgb, file)\n    \nwith open(\"Random Forrest.pkl\", 'wb') as file:\n    pickle.dump(rf, file)\n    \nwith open(\"Multi Layer Perceptron.pkl\", 'wb') as file:\n    pickle.dump(mlp, file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}